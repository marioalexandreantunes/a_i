# üñ• Processo de cria√ß√£o de uma Large/Small Language Model (LLM/SLM) do zero.

Este artigo mergulha no mundo da Intelig√™ncia Artificial (IA) e modelos de linguagem de grande escala (LLMs/SLMs), como o famoso GPT da OpenAI. 

LLMs/SLMs aprendem a imitar fun√ß√µes cognitivas humanas, identificando e replicando padr√µes em dados de texto.  Eles utilizam uma estrutura inovadora chamada "transformers" para treinar com alta efici√™ncia, prevendo a pr√≥xima palavra em uma sequ√™ncia com precis√£o impressionante.

O artigo explora tr√™s tipos principais de arquiteturas Transformer:

* **Encoder:** Para tarefas de compreens√£o de texto (ex: BERT, RoBERTa).
* **Decoder:** Para gera√ß√£o de texto (ex: GPT).
* **Encoder-Decoder:** Combinando ambas as funcionalidades (ex: T5).

Atrav√©s de exemplo pr√°tico, aprender√° a construir o seu pr√≥prio modelo de linguagem com Transformers. 

---

# üìÑ Pr√©-Processamento de Dados

O pr√©-processamento de dados para modelos de linguagem de grande escala (LLMs/SLMs) envolve v√°rias etapas detalhadas

1. Os dados s√£o divididos em pequenas unidades chamadas tokens, que podem ser palavras, subpalavras, n√∫meros ou s√≠mbolos.
2. Os dados s√£o limpos para remover erros, conte√∫do ofensivo ou spam. O texto √© ent√£o normalizado, o que pode incluir a convers√£o de todas as letras para min√∫sculas, remo√ß√£o de stopwords, e aplica√ß√£o de t√©cnicas de stemming ou lematiza√ß√£o.
3. Esses 'tokens' s√£o transformados em n√∫meros atrav√©s de t√©cnicas como embedding, para que o modelo possa process√°-los eficientemente.

> Um **token** √© uma unidade b√°sica de linguagem em um modelo de linguagem geradora, como GPT-2 ou BERT. √â uma palavra ou um s√≠mbolo que representa uma ideia ou uma informa√ß√£o espec√≠fica. Por exemplo, "O gato" √© um token, e "gato" √© um subtoken.

> Uma t√©cnica de **embedding** √© uma abordagem utilizada em machine learning para mapear objetos de diferentes dom√≠nios para um espa√ßo vetorial comum. Em outras palavras, ela permite que os modelos de linguagem geradora representem informa√ß√µes de diferentes fontes de dados em um formato comum e f√°cil de processar.

Lembre-se de que a cria√ß√£o de um **dataset** √© uma tarefa desafiadora e requer tempo e esfor√ßo. No entanto, com certeza e dedica√ß√£o, voc√™ pode criar um dataset √∫til para o treinamento do modelo de linguagem ou para outras tarefas espec√≠ficas.
Um **dataset** pode ser uma forma de tabela de perguntas e respostas e em forma JSON e dever√° ser em ingl√™s para obter melhores resultados.

Verifica se j√° n√£o h√° um dataset que possas melhorar e usar, https://huggingface.co/datasets

Exemplo ficheiro json com tema de futebol:
```
[
{
    "question": "What are the dimensions of a regulation soccer field?",
    "answer": "A regulation soccer field is 100-130 yards long and 50-100 yards wide."
  },
  {
    "question": "How many players are on the field for each team in a soccer match?",
    "answer": "Each team has 11 players on the field at a time, including the goalkeeper."
  },
  {
    "question": "What is an offside rule in soccer?",
    "answer": "A player is offside if they are closer to the opponent's goal line than both the ball and the second-to-last defender when the ball is played to them."
  },
  {
    "question": "What is a penalty kick in soccer?",
    "answer": "A penalty kick is awarded to the attacking team when a foul is committed by a defender inside the penalty area."
  },
  {
    "question": "What are the different positions in a soccer team?",
    "answer": "Common positions include goalkeeper, defenders (center-backs, full-backs), midfielders (central midfielders, wingers), and forwards (strikers)."
  }
]
```
---
# üß† Machine Learning

Para treinar um LLM/SLM, √© necess√°rio equipamento especializado, como GPUs ou TPUs de alto desempenho, para processar grandes volumes de dados e realizar c√°lculos complexos. Neste exemplo, utilizarei o **Google Colab**, que oferece acesso gratuito a GPUs e TPUs , possibilitando um treinamento sem a necessidade de investir em hardware caro.

> Um arquivo `.ipynb` no Google Colab √© um arquivo de notebook Jupyter. O Google Colab √© uma plataforma gratuita e em nuvem que permite a execu√ß√£o de c√≥digo Python, R e Julia em um ambiente de notebook interativo. Os arquivos `.ipynb` s√£o usados para armazenar e compartilhar c√≥digos Jupyter, que s√£o documentos que cont√™m c√≥digo, texto e visualiza√ß√µes.

## üî® Crie um Google Colab e habilite a T4 GPU

Aqui est√£o os passos para criar um Google Colab e habilitar a GPU T4:

### 1. Acesse o Google Colab:**

* V√° para o site do Google Colab: [https://colab.research.google.com/](https://colab.research.google.com/)

### 2. Crie um novo Notebook:**

* Clique no bot√£o "Novo" para criar um novo notebook.

### 3. Habilite a GPU T4:**

* **Clique no menu "Runtime" no topo da tela.**
* **Selecione "Change runtime type".**
* **Na janela que aparece, escolha "GPU" como o "Hardware accelerator".**
* **Selecione "Tesla T4" como o tipo de GPU.**
* **Clique em "Save".**

### 4. Verifique se a GPU est√° habilitada:**

* Execute o seguinte c√≥digo no seu notebook:

```python
  !nvidia-smi
```

* Isso exibir√° informa√ß√µes sobre a GPU T4 habilitada, incluindo o nome do modelo, mem√≥ria dispon√≠vel e uso da GPU.

**Observa√ß√µes:**

* A disponibilidade de GPUs T4 pode variar dependendo da demanda. Se voc√™ n√£o conseguir encontrar a op√ß√£o T4, tente escolher outra GPU dispon√≠vel, como a Tesla K80.
* O uso de GPUs pode resultar em custos adicionais, dependendo do tempo de execu√ß√£o e da quantidade de recursos utilizados.


## üî® Instala√ß√£o das bibliotecas necess√°rias

```batch
  pip install transformers[torch] datasets torch
```

A biblioteca transformers da Hugging Face oferece modelos de linguagem pr√©-treinados como GPT-2, BERT e T5 para tarefas como gera√ß√£o de texto, resposta a perguntas e tradu√ß√£o. √â f√°cil de usar e permite treinar ou ajustar modelos para necessidades espec√≠ficas. √â uma ferramenta essencial para aplicar intelig√™ncia artificial em linguagem natural. A biblioteca torch √© usada para computa√ß√£o e treinamento eficiente, e a biblioteca datasets √© usada para manipular nossos dados de treino.

## üî® Importa√ß√£o das bibliotecas

Importa√ß√£o das bibliotecas
```python
  from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments
  from datasets import Dataset
  import json
  import pandas as pd
```

Neste passo, importamos as bibliotecas essenciais para manipula√ß√£o de dados e deep learning. Utilizamos o torch para opera√ß√µes de tensor e computa√ß√£o em GPU.
A biblioteca transformers para carregar e treinar o modelo GPT-2, incluindo GPT2Tokenizer, GPT2LMHeadModel, Trainer e TrainingArguments.
O datasets do Hugging Face para preparar e manipular conjuntos de dados.
A biblioteca padr√£o json para leitura de arquivos JSON que √© o nosso ficheiro de dados, e o pandas para transformar dados JSON em DataFrames para an√°lise e manipula√ß√£o eficientes. Essas bibliotecas s√£o fundamentais para realizar tarefas de processamento de linguagem natural (NLP) de maneira eficaz.

## üî® Carregar dados do arquivo JSON
```python
  def load_data(file_path):
    with open(file_path, 'r') as file:
    data = json.load(file)
    return pd.DataFrame(data)
  
  file_path = '/content/dataset.json'
  df = load_data(file_path)
```

No colab ao lado esquerdo folder icon, '/content' √© o root, carrega o ficheiro para l√°!

O objetivo desta etapa √© carregar os dados contidos em um arquivo JSON e convert√™-los em um DataFrame do Pandas. 
Um DataFrame √© uma estrutura de dados bidimensional que facilita a manipula√ß√£o e an√°lise dos dados.
A Cria√ß√£o do JSON que contem perguntas e respostas podes criar usando uma AI. Utiliza uma s√©rie de prompts para gerar JSONs contendo perguntas e respostas sobre uma ampla gama de t√≥picos que desejas treinar o teu modelo. 
Para garantir a diversidade das informa√ß√µes, usa [crewAI](https://www.crewai.com/) onde ter√°s liga√ß√£o a modelos OpenGPT, Gemini, Claude etc
Verifica depois se tens ou n√£o perguntas repetidas! √â muito importante n√£o ter perguntas repetidas. Possivelmente ter√°s de criar um c√≥digo/script que fa√ßa isso automaticamente.
Nesta fase √© onde ter√°s de ler e aprender a ter os dados necess√°rios e correctos. O Fine Tuning usar√° tamb√©m datasets!

## üî® Inicializar tokenizador e preparar dados  

```python
	tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')
	tokenizer.pad_token = tokenizer.eos_token

	df['text'] = df['question'] + " " + df['answer']

	def tokenize_function(examples):
    		return tokenizer(examples, padding="max_length", truncation=True, max_length=512)

	tokenized_datasets = df['text'].apply(lambda x: tokenize_function(x))
	dataset = Dataset.from_pandas(df)

	def tokenize_dataset(dataset):
    		return dataset.map(lambda x: tokenizer(x['text'], padding="max_length", truncation=True, max_length=512), batched=True, remove_columns=["text", "question", "answer"])

	tokenized_dataset = tokenize_dataset(dataset)
```

**[distilgpt2](https://huggingface.co/distilbert/distilgpt2)** √© um modelo de l√≠ngua **inglesa pr√©-treinado** gerador de texto baseado no modelo GPT-2 (Generative Pre-trained Transformer 2), que √© uma das melhores op√ß√µes para a gera√ß√£o de texto natural. Ele foi projetado para ser mais eficiente e f√°cil de usar do que o modelo original, tornando-o uma op√ß√£o popular entre os desenvolvedores que buscam treinar modelos de linguagem Decoders (GPT). Exitem outros modelos como BART, ELECTRA ou T5.

Neste passo, inicializamos o tokenizador GPT2Tokenizer do modelo **distilgpt2** e configuramos o token de padding para ser o mesmo que o token de fim de sequ√™ncia (EOS). Concatenamos perguntas e respostas em uma nova coluna text no DataFrame e definimos uma fun√ß√£o de tokeniza√ß√£o que aplica padding e truncamento at√© um comprimento m√°ximo de 512 tokens. Aplicamos esta fun√ß√£o de tokeniza√ß√£o a cada texto no DataFrame, convertendo-o em tokens. Em seguida, transformamos o DataFrame em um Dataset do Hugging Face, facilitando o processamento eficiente e removendo colunas originais ap√≥s a tokeniza√ß√£o para otimiza√ß√£o.
O token de padding √© utilizado para garantir que todas as sequ√™ncias de entrada em um lote (batch) de dados tenham o mesmo comprimento. Isso √© necess√°rio porque os modelos de deep learning, como os Transformers, requerem que as entradas tenham dimens√µes consistentes para processamento eficiente em paralelo.

## üî® Adicionar labels e dividir dataset

codigo completo :
```python
	tokenized_dataset = tokenized_dataset.map(lambda examples: {'labels': examples['input_ids']}, batched=True)

	train_test_split = tokenized_dataset.train_test_split(test_size=0.15)
	train_dataset = train_test_split['train']
	test_dataset = train_test_split['test']
```

## üî® Explica√ß√£o detalhada da etapa de adi√ß√£o de labels e divis√£o do dataset:

Este passo √© fundamental para preparar os dados para o treinamento do modelo de linguagem. 

### 1. Adicionando Labels:

```python
tokenized_dataset = tokenized_dataset.map(lambda examples: {'labels': examples['input_ids']}, batched=True)
```

* **Objetivo:** Atribuir labels aos dados tokenizados. 
* **Como funciona:**
    * `tokenized_dataset.map`: Aplica uma fun√ß√£o a cada exemplo do dataset.
    * `lambda examples: {'labels': examples['input_ids']}`:  Fun√ß√£o que cria um novo dicion√°rio para cada exemplo, adicionando uma chave 'labels' com o valor de 'input_ids'.  Os 'input_ids' representam as representa√ß√µes num√©ricas das palavras no texto, que ser√£o usadas como labels para o modelo aprender a prever as pr√≥ximas palavras.
    * `batched=True`: Processa os exemplos em batches para otimizar o desempenho.

### 2. Dividindo o Dataset:

```python
train_test_split = tokenized_dataset.train_test_split(test_size=0.15)
train_dataset = train_test_split['train']
test_dataset = train_test_split['test']
```

* **Objetivo:** Separar o dataset em conjuntos de treino e teste.
* **Como funciona:**
    * `tokenized_dataset.train_test_split(test_size=0.15)`: Divide o dataset em 85% para treino e 15% para teste.
    * `train_dataset = train_test_split['train']`: Atribui o conjunto de treino √† vari√°vel `train_dataset`.
    * `test_dataset = train_test_split['test']`: Atribui o conjunto de teste √† vari√°vel `test_dataset`.

**Import√¢ncia da Divis√£o:**

* **Preven√ß√£o de Overfitting:** O modelo aprende melhor com dados que ele n√£o viu durante o treinamento. O conjunto de teste garante que o modelo seja avaliado com dados n√£o vistos, evitando que ele memorize o conjunto de treino e se torne incapaz de generalizar para novos dados.
* **Avalia√ß√£o do Desempenho:** O conjunto de teste permite avaliar o desempenho do modelo em dados reais e comparar diferentes modelos.


## üî® Fun√ß√£o de agrupamento de dados

```python
	def data_collator(features):
    		batch = {}
    		batch['input_ids'] = torch.tensor([f['input_ids'] for f in features], dtype=torch.long)
    		batch['attention_mask'] = torch.tensor([f['attention_mask'] for f in features], dtype=torch.long)
    		batch['labels'] = torch.tensor([f['labels'] for f in features], dtype=torch.long)
    		return batch
```

## üî® A fun√ß√£o `data_collator`:  Organizando os dados para o treinamento eficiente

A fun√ß√£o `data_collator` organiza dados tokenizados em batches para treinamento eficiente de modelos de linguagem. Ela converte listas de tokens, m√°scaras de aten√ß√£o e r√≥tulos em tensores PyTorch, garantindo que todos os batches tenham a mesma estrutura atrav√©s de t√©cnicas de padding. Essa organiza√ß√£o facilita o processamento computacional, garante consist√™ncia no treinamento e permite uma integra√ß√£o simplificada com o Hugging Face Trainer. 

> Tensores PyTorch s√£o estruturas de dados fundamentais similares aos arrays multidimensionais mas com a vantagem adicional de serem otimizados para opera√ß√µes de alto desempenho em GPUs (unidades de processamento gr√°fico).

## üî® Inicializar modelo e argumentos de treinamento

```python
	model = GPT2LMHeadModel.from_pretrained('distilgpt2')
	
	training_args = TrainingArguments(
	    output_dir='./results',
	    eval_strategy="epoch",
	    learning_rate=3e-5,
	    per_device_train_batch_size=8,
	    per_device_eval_batch_size=8,
	    num_train_epochs=5,
	    weight_decay=0.01,
	    logging_dir='./logs',
	    logging_steps=10,
	)
```

Neste passo, preparamos o ambiente para o treinamento do modelo GPT-2. 

* **Carregamos o modelo:** Utilizamos `GPT2LMHeadModel.from_pretrained('distilgpt2')` para carregar uma vers√£o otimizada e menor do GPT-2, chamada distilgpt2.

* **Configuramos o treinamento:** Definimos os par√¢metros de treinamento usando `TrainingArguments`, controlando aspectos como:
    * **Sa√≠da:** Diret√≥rio para salvar os resultados do treinamento.
    * **Avalia√ß√£o:** Estrat√©gia para avaliar o desempenho do modelo.
    * **Taxa de aprendizado:**  Velocidade de ajuste dos par√¢metros do modelo.
    * **Tamanho do batch:** Quantidade de dados processados por vez.
    * **N√∫mero de √©pocas:**  N√∫mero de vezes que todo o dataset √© usado para treinamento.
    * **Decaimento de peso:**  Redu√ß√£o gradual da taxa de aprendizado durante o treinamento.
    * **Logs:** Diret√≥rio para salvar informa√ß√µes sobre o progresso do treinamento.
    * **Frequ√™ncia de registro:**  Intervalo de tempo para registrar informa√ß√µes sobre o treinamento.

* **Iniciando o treinamento:**  O `Trainer` utiliza esses argumentos, juntamente com os datasets e a fun√ß√£o de agrupamento de dados (`data_collator`), para gerenciar o processo de treinamento e avalia√ß√£o do modelo de forma eficiente e controlada.









