# üñ• Processo de cria√ß√£o de uma Large/Small Language Model (LLM/SLM) do zero.

Este artigo mergulha no mundo da Intelig√™ncia Artificial (AI) e modelos de linguagem de grande escala (LLMs/SLMs), como o famoso GPT da OpenAI. 

LLMs/SLMs aprendem a imitar fun√ß√µes cognitivas humanas, identificando e replicando padr√µes em dados de texto.  Eles utilizam uma estrutura inovadora chamada "transformers" para treinar com alta efici√™ncia, prevendo a pr√≥xima palavra em uma sequ√™ncia com precis√£o impressionante.

O artigo explora tr√™s tipos principais de arquiteturas Transformer:

* **Encoder:** Para tarefas de compreens√£o de texto (ex: BERT, RoBERTa).
* **Decoder:** Para gera√ß√£o de texto (ex: GPT).
* **Encoder-Decoder:** Combinando ambas as funcionalidades (ex: T5).

Atrav√©s de exemplo pr√°tico, aprender√° a construir o seu pr√≥prio modelo de linguagem com Transformers. 

---

# üìÑ Pr√©-Processamento de Dados

O pr√©-processamento de dados para modelos de linguagem de grande escala (LLMs/SLMs) envolve v√°rias etapas detalhadas

1. Os dados s√£o divididos em pequenas unidades chamadas tokens, que podem ser palavras, subpalavras, n√∫meros ou s√≠mbolos.
2. Os dados s√£o limpos para remover erros, conte√∫do ofensivo ou spam. O texto √© ent√£o normalizado, o que pode incluir a convers√£o de todas as letras para min√∫sculas, remo√ß√£o de stopwords, e aplica√ß√£o de t√©cnicas de stemming ou lematiza√ß√£o.
3. Esses 'tokens' s√£o transformados em n√∫meros atrav√©s de t√©cnicas como embedding, para que o modelo possa process√°-los eficientemente.

> [!NOTE]
> Um **token** √© uma unidade b√°sica de linguagem em um modelo de linguagem geradora, como GPT-2 ou BERT. √â uma palavra ou um s√≠mbolo que representa uma ideia ou uma informa√ß√£o espec√≠fica. Por exemplo, "O gato" √© um token, e "gato" √© um subtoken.

> [!NOTE]
> Uma t√©cnica de **embedding** √© uma abordagem utilizada em machine learning para mapear objetos de diferentes dom√≠nios para um espa√ßo vetorial comum. Em outras palavras, ela permite que os modelos de linguagem geradora representem informa√ß√µes de diferentes fontes de dados em um formato comum e f√°cil de processar.

Lembre-se de que a cria√ß√£o de um **dataset** √© uma tarefa desafiadora e requer tempo e esfor√ßo. No entanto, com certeza e dedica√ß√£o, voc√™ pode criar um dataset √∫til para o treinamento do modelo de linguagem ou para outras tarefas espec√≠ficas.
Um **dataset** pode ser uma forma de tabela de perguntas e respostas e em forma JSON e dever√° ser em ingl√™s para obter melhores resultados.

Verifica se j√° n√£o h√° um dataset que possas melhorar e usar, https://huggingface.co/datasets

üëâ [YouTube](https://www.youtube.com/results?search_query=como+fazer+datasets+para+llm) mais videos sobre datasets

Exemplo ficheiro json com tema de futebol:
```
[
{
    "question": "What are the dimensions of a regulation soccer field?",
    "answer": "A regulation soccer field is 100-130 yards long and 50-100 yards wide."
  },
  {
    "question": "How many players are on the field for each team in a soccer match?",
    "answer": "Each team has 11 players on the field at a time, including the goalkeeper."
  },
  {
    "question": "What is an offside rule in soccer?",
    "answer": "A player is offside if they are closer to the opponent's goal line than both the ball and the second-to-last defender when the ball is played to them."
  },
  {
    "question": "What is a penalty kick in soccer?",
    "answer": "A penalty kick is awarded to the attacking team when a foul is committed by a defender inside the penalty area."
  },
  {
    "question": "What are the different positions in a soccer team?",
    "answer": "Common positions include goalkeeper, defenders (center-backs, full-backs), midfielders (central midfielders, wingers), and forwards (strikers)."
  }
]
```

---

# üß† Machine Learning

Para treinar um LLM/SLM, √© necess√°rio equipamento especializado, como GPUs ou TPUs de alto desempenho, para processar grandes volumes de dados e realizar c√°lculos complexos. Neste exemplo, utilizarei o **Google Colab**, que oferece acesso gratuito a GPUs e TPUs , possibilitando um treinamento sem a necessidade de investir em hardware caro.

> Um arquivo `.ipynb` no Google Colab √© um arquivo de notebook Jupyter. O Google Colab √© uma plataforma gratuita e em nuvem que permite a execu√ß√£o de c√≥digo Python, R e Julia em um ambiente de notebook interativo. Os arquivos `.ipynb` s√£o usados para armazenar e compartilhar c√≥digos Jupyter, que s√£o documentos que cont√™m c√≥digo, texto e visualiza√ß√µes.

## üî® Crie um Google Colab e habilite a T4 GPU

Aqui est√£o os passos para criar um Google Colab e habilitar a GPU T4:

### 1. Acesse o Google Colab:

* V√° para o site do Google Colab: [https://colab.research.google.com/](https://colab.research.google.com/)

### 2. Crie um novo Notebook:

* Clique no bot√£o "Novo" para criar um novo notebook.

### 3. Habilite a GPU T4:

* **Clique no menu "Runtime" no topo da tela.**
* **Selecione "Change runtime type".**
* **Na janela que aparece, escolha "GPU" como o "Hardware accelerator".**
* **Selecione "Tesla T4" como o tipo de GPU.**
* **Clique em "Save".**

### 4. Verifique se a GPU est√° habilitada:

* Execute o seguinte c√≥digo no seu notebook:

```python
  !nvidia-smi
```

* Isso exibir√° informa√ß√µes sobre a GPU T4 habilitada, incluindo o nome do modelo, mem√≥ria dispon√≠vel e uso da GPU.

**Observa√ß√µes:**

* A disponibilidade de GPUs T4 pode variar dependendo da demanda. Se voc√™ n√£o conseguir encontrar a op√ß√£o T4, tente escolher outra GPU dispon√≠vel, como a Tesla K80.
* O uso de GPUs pode resultar em custos adicionais, dependendo do tempo de execu√ß√£o e da quantidade de recursos utilizados.


## üî® Instala√ß√£o das bibliotecas necess√°rias

```batch
	pip install transformers[torch] datasets torch
```

A biblioteca transformers da Hugging Face oferece modelos de linguagem pr√©-treinados como GPT-2, BERT e T5 para tarefas como gera√ß√£o de texto, resposta a perguntas e tradu√ß√£o. √â f√°cil de usar e permite treinar ou ajustar modelos para necessidades espec√≠ficas. √â uma ferramenta essencial para aplicar intelig√™ncia artificial em linguagem natural. A biblioteca torch √© usada para computa√ß√£o e treinamento eficiente, e a biblioteca datasets √© usada para manipular nossos dados de treino.

Aqui est√° um resumo dos modelos mais populares da biblioteca Transformers da Hugging Face:

1. BERT (Bidirectional Encoder Representations from Transformers)
	- **Descri√ß√£o**: Um modelo pr√©-treinado bidirecional para NLP, especializado em compreens√£o contextual.
	- **Usos**: Classifica√ß√£o de texto, resposta a perguntas, reconhecimento de entidades nomeadas.
	- **Exemplo**: `bert-base-uncased`

2. GPT (Generative Pre-trained Transformer)
	- **Descri√ß√£o**: Um modelo de gera√ß√£o de texto autoregressivo.
	- **Usos**: Gera√ß√£o de texto, completamento de texto, chatbots.
	- **Exemplo**: `gpt2`, `gpt-3.5-turbo`

3. RoBERTa (A Robustly Optimized BERT Pretraining Approach)
	- **Descri√ß√£o**: Uma variante de BERT otimizada com mais dados e ajustes de hiperpar√¢metros.
	- **Usos**: Tarefas semelhantes ao BERT, com melhor desempenho.
	- **Exemplo**: `roberta-base`

4. DistilBERT (Distilled version of BERT)
	- **Descri√ß√£o**: Uma vers√£o compacta de BERT que √© mais r√°pida e eficiente.
	- **Usos**: Tarefas de NLP com menor necessidade computacional.
	- **Exemplo**: `distilbert-base-uncased`

5. T5 (Text-To-Text Transfer Transformer)
	- **Descri√ß√£o**: Um modelo que trata todas as tarefas de NLP como problemas de tradu√ß√£o de texto-para-texto.
	- **Usos**: Tradu√ß√£o, sumariza√ß√£o, gera√ß√£o de texto.
	- **Exemplo**: `t5-small`, `t5-base`

6. XLNet
	- **Descri√ß√£o**: Um modelo autoregressivo que tamb√©m captura depend√™ncias bidirecionais.
	- **Usos**: Modelos de linguagem com predi√ß√£o bidirecional.
	- **Exemplo**: `xlnet-base-cased`

7. ALBERT (A Lite BERT)
	- **Descri√ß√£o**: Uma vers√£o leve de BERT com menos par√¢metros e arquitetura eficiente.
	- **Usos**: Tarefas de NLP com menor consumo de mem√≥ria e mais r√°pidas.
	- **Exemplo**: `albert-base-v2`

8. Bart (Bidirectional and Auto-Regressive Transformers)
	- **Descri√ß√£o**: Um modelo que combina as capacidades de modelos bidirecionais e autoregressivos.
	- **Usos**: Tradu√ß√£o, sumariza√ß√£o, gera√ß√£o de texto.
	- **Exemplo**: `facebook/bart-large`

9. Electra (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)
	- **Descri√ß√£o**: Um modelo eficiente que aprende a discriminar entre tokens reais e substitu√≠dos.
	- **Usos**: Pr√©-treino eficiente para tarefas de NLP.
	- **Exemplo**: `google/electra-small-discriminator`

10. BERTweet
	- **Descri√ß√£o**: Um modelo baseado em BERT treinado especificamente em dados do Twitter.
	- **Usos**: An√°lise de sentimentos, classifica√ß√£o de texto em tweets.
	- **Exemplo**: `vinai/bertweet-base`

## üî® Importa√ß√£o das bibliotecas

Importa√ß√£o das bibliotecas
```python
	  from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments
	  from datasets import Dataset
	  import json
	  import pandas as pd
```

[LINK](https://huggingface.co/docs/transformers/v4.43.3/en/model_doc/gpt2)

Neste passo, importamos as bibliotecas essenciais para manipula√ß√£o de dados e deep learning. Utilizamos o torch para opera√ß√µes de tensor e computa√ß√£o em GPU.
A biblioteca transformers para carregar e treinar o modelo GPT-2, incluindo GPT2Tokenizer, GPT2LMHeadModel, Trainer e TrainingArguments.
O datasets do Hugging Face para preparar e manipular conjuntos de dados.
A biblioteca padr√£o json para leitura de arquivos JSON que √© o nosso ficheiro de dados, e o pandas para transformar dados JSON em DataFrames para an√°lise e manipula√ß√£o eficientes. Essas bibliotecas s√£o fundamentais para realizar tarefas de processamento de linguagem natural (NLP) de maneira eficaz.

## üî® Carregar dados do arquivo JSON
```python
	  def load_data(file_path):
	    with open(file_path, 'r') as file:
	    data = json.load(file)
	    return pd.DataFrame(data)
	  
	  file_path = '/content/dataset.json'
	  df = load_data(file_path)
```

No colab ao lado esquerdo folder icon, '/content' √© o root, carrega o ficheiro para l√°!

O objetivo desta etapa √© carregar os dados contidos em um arquivo JSON e convert√™-los em um DataFrame do Pandas. 
Um DataFrame √© uma estrutura de dados bidimensional que facilita a manipula√ß√£o e an√°lise dos dados.
A Cria√ß√£o do JSON que contem perguntas e respostas podes criar usando uma AI. Utiliza uma s√©rie de prompts para gerar JSONs contendo perguntas e respostas sobre uma ampla gama de t√≥picos que desejas treinar o teu modelo. 
Para garantir a diversidade das informa√ß√µes, usa [crewAI](https://www.crewai.com/) onde ter√°s liga√ß√£o a modelos OpenGPT, Gemini, Claude etc
Verifica depois se tens ou n√£o perguntas repetidas! √â muito importante n√£o ter perguntas repetidas. Possivelmente ter√°s de criar um c√≥digo/script que fa√ßa isso automaticamente.
Nesta fase √© onde ter√°s de ler e aprender a ter os dados necess√°rios e correctos. O Fine Tuning usar√° tamb√©m datasets!

## üî® Inicializar tokenizador e preparar dados  

```python
	tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')
	tokenizer.pad_token = tokenizer.eos_token

	df['text'] = df['question'] + " " + df['answer']

	def tokenize_function(examples):
    		return tokenizer(examples, padding="max_length", truncation=True, max_length=512)

	tokenized_datasets = df['text'].apply(lambda x: tokenize_function(x))
	dataset = Dataset.from_pandas(df)

	def tokenize_dataset(dataset):
    		return dataset.map(lambda x: tokenizer(x['text'], padding="max_length", truncation=True, max_length=512), batched=True, remove_columns=["text", "question", "answer"])

	tokenized_dataset = tokenize_dataset(dataset)
```

A biblioteca Transformers da Hugging Face oferece uma ampla variedade de modelos pr√©-treinados que voc√™ pode utilizar com o `GPT2Tokenizer.from_pretrained`. Aqui est√£o **algumas** op√ß√µes:

1. **`gpt2`** : O modelo original GPT-2, treinado em um conjunto de dados de 45 GB de texto.
2. **`gpt2-medium`** : Uma vers√£o m√©dia do modelo GPT-2, treinada em um conjunto de dados de 10 GB de texto.
3. **`gpt2-large`** : Uma vers√£o grande do modelo GPT-2, treinada em um conjunto de dados de 300 GB de texto.
4. **`gpt2-xl`** : Uma vers√£o extra grande do modelo GPT-2, treinada em um conjunto de dados de 1,2 TB de texto.
5. **`gpt2-125M`** : Uma vers√£o do modelo GPT-2 com 125 milh√µes de par√¢metros, treinada em um conjunto de dados de 10 GB de texto.
6. **`gpt2-355M`** : Uma vers√£o do modelo GPT-2 com 355 milh√µes de par√¢metros, treinada em um conjunto de dados de 10 GB de texto.
7. **`gpt2-774M`** : Uma vers√£o do modelo GPT-2 com 774 milh√µes de par√¢metros, treinada em um conjunto de dados de 10 GB de texto.
8. **`gpt2-125M-uncased`** : Uma vers√£o do modelo GPT-2 com 125 milh√µes de par√¢metros, treinada em um conjunto de dados de 10 GB de texto e sem case sensitivity.
9. **`gpt2-355M-uncased`** : Uma vers√£o do modelo GPT-2 com 355 milh√µes de par√¢metros, treinada em um conjunto de dados de 10 GB de texto e sem case sensitivity.
10. **`gpt2-774M-uncased`** : Uma vers√£o do modelo GPT-2 com 774 milh√µes de par√¢metros, treinada em um conjunto de dados de 10 GB de texto e sem case sensitivity.

Lembre-se de que cada modelo pr√©-treinado tem suas pr√≥prias caracter√≠sticas e habilidades, e voc√™ deve escolher o modelo que melhor se adequare √†s suas necessidades espec√≠ficas.

**[distilgpt2](https://huggingface.co/distilbert/distilgpt2)** √© um modelo de l√≠ngua **inglesa pr√©-treinado** gerador de texto baseado no modelo GPT-2 (Generative Pre-trained Transformer 2), que √© uma das melhores op√ß√µes para a gera√ß√£o de texto natural. Ele foi projetado para ser mais eficiente e f√°cil de usar do que o modelo original, tornando-o uma op√ß√£o popular entre os desenvolvedores que buscam treinar modelos de linguagem Decoders (GPT).

Neste passo, inicializamos o tokenizador GPT2Tokenizer do modelo **distilgpt2** e configuramos o token de padding para ser o mesmo que o token de fim de sequ√™ncia (EOS). Concatenamos perguntas e respostas em uma nova coluna text no DataFrame e definimos uma fun√ß√£o de tokeniza√ß√£o que aplica padding e truncamento at√© um comprimento m√°ximo de 512 tokens. Aplicamos esta fun√ß√£o de tokeniza√ß√£o a cada texto no DataFrame, convertendo-o em tokens. Em seguida, transformamos o DataFrame em um Dataset do Hugging Face, facilitando o processamento eficiente e removendo colunas originais ap√≥s a tokeniza√ß√£o para otimiza√ß√£o.
O token de padding √© utilizado para garantir que todas as sequ√™ncias de entrada em um lote (batch) de dados tenham o mesmo comprimento. Isso √© necess√°rio porque os modelos de deep learning, como os Transformers, requerem que as entradas tenham dimens√µes consistentes para processamento eficiente em paralelo.

## üî® Adicionar labels e dividir dataset

codigo completo :
```python
	tokenized_dataset = tokenized_dataset.map(lambda examples: {'labels': examples['input_ids']}, batched=True)

	train_test_split = tokenized_dataset.train_test_split(test_size=0.15)
	train_dataset = train_test_split['train']
	test_dataset = train_test_split['test']
```

## üî® Explica√ß√£o detalhada da etapa de adi√ß√£o de labels e divis√£o do dataset:

Este passo √© fundamental para preparar os dados para o treinamento do modelo de linguagem. 

### 1. Adicionando Labels:

```python
tokenized_dataset = tokenized_dataset.map(lambda examples: {'labels': examples['input_ids']}, batched=True)
```

* **Objetivo:** Atribuir labels aos dados tokenizados. 
* **Como funciona:**
    * `tokenized_dataset.map`: Aplica uma fun√ß√£o a cada exemplo do dataset.
    * `lambda examples: {'labels': examples['input_ids']}`:  Fun√ß√£o que cria um novo dicion√°rio para cada exemplo, adicionando uma chave 'labels' com o valor de 'input_ids'.  Os 'input_ids' representam as representa√ß√µes num√©ricas das palavras no texto, que ser√£o usadas como labels para o modelo aprender a prever as pr√≥ximas palavras.
    * `batched=True`: Processa os exemplos em batches para otimizar o desempenho.

### 2. Dividindo o Dataset:

```python
train_test_split = tokenized_dataset.train_test_split(test_size=0.15)
train_dataset = train_test_split['train']
test_dataset = train_test_split['test']
```

* **Objetivo:** Separar o dataset em conjuntos de treino e teste.
* **Como funciona:**
    * `tokenized_dataset.train_test_split(test_size=0.15)`: Divide o dataset em 85% para treino e 15% para teste.
    * `train_dataset = train_test_split['train']`: Atribui o conjunto de treino √† vari√°vel `train_dataset`.
    * `test_dataset = train_test_split['test']`: Atribui o conjunto de teste √† vari√°vel `test_dataset`.

**Import√¢ncia da Divis√£o:**

* **Preven√ß√£o de Overfitting:** O modelo aprende melhor com dados que ele n√£o viu durante o treinamento. O conjunto de teste garante que o modelo seja avaliado com dados n√£o vistos, evitando que ele memorize o conjunto de treino e se torne incapaz de generalizar para novos dados.
* **Avalia√ß√£o do Desempenho:** O conjunto de teste permite avaliar o desempenho do modelo em dados reais e comparar diferentes modelos.


## üî® Fun√ß√£o de agrupamento de dados

```python
	def data_collator(features):
    		batch = {}
    		batch['input_ids'] = torch.tensor([f['input_ids'] for f in features], dtype=torch.long)
    		batch['attention_mask'] = torch.tensor([f['attention_mask'] for f in features], dtype=torch.long)
    		batch['labels'] = torch.tensor([f['labels'] for f in features], dtype=torch.long)
    		return batch
```

## üî® A fun√ß√£o `data_collator`:  Organizando os dados para o treinamento eficiente

A fun√ß√£o `data_collator` organiza dados tokenizados em batches para treinamento eficiente de modelos de linguagem. Ela converte listas de tokens, m√°scaras de aten√ß√£o e r√≥tulos em tensores PyTorch, garantindo que todos os batches tenham a mesma estrutura atrav√©s de t√©cnicas de padding. Essa organiza√ß√£o facilita o processamento computacional, garante consist√™ncia no treinamento e permite uma integra√ß√£o simplificada com o Hugging Face Trainer. 

> [!NOTE]
> Tensores PyTorch s√£o estruturas de dados fundamentais similares aos arrays multidimensionais mas com a vantagem adicional de serem otimizados para opera√ß√µes de alto desempenho em GPUs (unidades de processamento gr√°fico).

## üî® Inicializar modelo e argumentos de treinamento

```python
	model = GPT2LMHeadModel.from_pretrained('distilgpt2')
	
	training_args = TrainingArguments(
	    output_dir='./results',
	    eval_strategy="epoch",
	    learning_rate=3e-5,
	    per_device_train_batch_size=8,
	    per_device_eval_batch_size=8,
	    num_train_epochs=5,
	    weight_decay=0.01,
	    logging_dir='./logs',
	    logging_steps=10,
	)
```

Neste passo, preparamos o ambiente para o treinamento do modelo GPT-2. 

Quando voc√™ executa o comando `model = GPT2LMHeadModel.from_pretrained('distilgpt2')`, voc√™ est√° carregando o modelo de linguagem GPT-2, que √© uma rede neural treinada para prever as pr√≥ximas palavras em uma sequ√™ncia de texto.

Aqui est√£o alguns dos par√¢metros do modelo GPT-2:

* **N√∫mero de camadas**: 12
* **N√∫mero de neur√¥nios por camada**: 768
* **Tamanho da janela de aten√ß√£o**: 512
* **N√∫mero de heads de aten√ß√£o**: 12
* **Taxa de aprendizado**: 1e-4
* **N√∫mero de √©pocas de treinamento**: 30
* **Tamanho do batch**: 32
* **Regulariza√ß√£o**: L2 regularization com um peso de 0.1
* **Dropout**: 0.1

√â importante notar que esses par√¢metros s√£o os padr√µes utilizados durante o treinamento do modelo GPT-2. Voc√™ pode ajustar esses par√¢metros para melhorar o desempenho do modelo em seu problema espec√≠fico.

Se voc√™ quiser saber mais sobre os par√¢metros do modelo GPT-2, recomendo que voc√™ leia o artigo original sobre o modelo GPT-2, publicado na arXiv em 2019.

* **Configuramos o treinamento:** Definimos os par√¢metros de treinamento usando `TrainingArguments`, controlando aspectos como:
    * **Sa√≠da:** Diret√≥rio para salvar os resultados do treinamento.
    * **Avalia√ß√£o:** Estrat√©gia para avaliar o desempenho do modelo.
    * **Taxa de aprendizado:**  Velocidade de ajuste dos par√¢metros do modelo.
    * **Tamanho do batch:** Quantidade de dados processados por vez.
    * **N√∫mero de passagens:**  N√∫mero de vezes que todo o dataset √© usado para treinamento.
    * **Decaimento de peso:**  Redu√ß√£o gradual da taxa de aprendisagem durante o treinamento.
    * **Logs:** Diret√≥rio para salvar informa√ß√µes sobre o progresso do treinamento.
    * **Frequ√™ncia de registro:**  Intervalo de tempo para registrar informa√ß√µes sobre o treinamento.

Alguns frameworks de machine learning, como TensorFlow e PyTorch, oferecem suporte √† **quantiza√ß√£o** de pesos e ativamentos em modelos de Transformer.
A quantiza√ß√£o √© uma t√©cnica usada para reduzir a precis√£o dos n√∫meros que representam os pesos e ativamentos de uma rede neural. Em vez de usar n√∫meros de 32 bits (como √© comum em muitas implementa√ß√µes), os valores s√£o representados com menos bits, como 8 bits ou at√© 4 bits. Isso tem v√°rias vantagens:

1. Redu√ß√£o do tamanho do modelo: Um modelo quantizado ocupa menos espa√ßo na mem√≥ria, o que √© √∫til para implementa√ß√µes em dispositivos com recursos limitados, como smartphones ou dispositivos IoT.
2. Melhoria na velocidade de infer√™ncia: Opera√ß√µes com n√∫meros de menor precis√£o podem ser computadas mais rapidamente, o que acelera o tempo de infer√™ncia.
3. Menor consumo de energia: Modelos quantizados consomem menos energia, o que √© importante para dispositivos m√≥veis e outros sistemas embarcados.

No Hugging Face e outras plataformas de modelagem, voc√™ pode ver modelos quantizados em 8 bits ou 4 bits usados para tarefas como processamento de linguagem natural, vis√£o computacional, entre outros. Estes modelos s√£o frequentemente utilizados em cen√°rios onde a efici√™ncia e a velocidade s√£o cruciais, e onde os dispositivos de execu√ß√£o t√™m limita√ß√µes de mem√≥ria e poder de processamento.

* **Iniciando o treinamento:**  O `Trainer` utiliza esses argumentos, juntamente com os datasets e a fun√ß√£o de agrupamento de dados (`data_collator`), para gerenciar o processo de treinamento e avalia√ß√£o do modelo de forma eficiente e controlada.

## üî® Configurar o trainer e iniciar o treinamento

```python
	trainer = Trainer(
	    model=model,
	    args=training_args,
	    train_dataset=train_dataset,
	    eval_dataset=test_dataset,
	    data_collator=data_collator
	)
	
	trainer.train()
```

Este passo envolve a configura√ß√£o do objeto Trainer da biblioteca transformers.

   - model=model: Especifica o modelo de aprendizado de m√°quina que ser√° treinado. Esse modelo j√° deve estar previamente carregado e configurado que ser√° **distilgpt2**
   - args=training_args: Configura√ß√µes e par√¢metros de treinamento, geralmente um objeto da classe TrainingArguments. Isso pode incluir informa√ß√µes como n√∫mero de √©pocas, tamanhos de lote (batch size), taxas de aprendizado e dispositivos a serem usados (CPU/GPU).
   - train_dataset=train_dataset: O conjunto de dados que ser√° usado para treinar o modelo.
   - eval_dataset=test_dataset: O conjunto de dados que ser√° usado para avaliar o desempenho do modelo durante o treinamento.
   - data_collator=data_collator: Um objeto que identifica como os dados devem ser agrupados em lotes (batches) durante o treinamento e a avalia√ß√£o.

Ap√≥s a configura√ß√£o, o treinamento √© iniciado com `trainer.train()`.

Etapas de treino s√£o forward pass >> c√°lculo da perda >> backward pass >> atualiza√ß√£o dos par√¢metros

> [!NOTE]
> O forward pass √© a etapa em que os dados de entrada s√£o passados pela rede neural, camada por camada, at√© que uma previs√£o (ou sa√≠da) seja gerada. Ele transforma inputs em outputs. Imagine que voc√™ est√° fornecendo ao modelo uma frase, como "O gato est√° dormindo". O modelo l√™ a frase e tenta prever a pr√≥xima palavra na frase, com base nas palavras que viu antes. Isso √© chamado de **forward pass** porque o modelo est√° se movendo para frente, processando a frase de entrada e fazendo previs√µes.

> [!NOTE]
> O c√°lculo da perda quantifica o erro das predi√ß√µes da rede comparado aos valores reais, utilizando fun√ß√µes de perda espec√≠ficas. Este valor √© crucial para ajustar os pesos da rede e melhorar a precis√£o do modelo, mede o qu√£o distante as predi√ß√µes da rede est√£o dos valores reais. Se o modelo prev√™ a palavra correta, a perda √© baixa. Se ele prev√™ uma palavra errada, a perda √© alta. O objetivo √© minimizar a perda, o que significa que o modelo est√° melhorando para prever a pr√≥xima palavra.

> [!NOTE]
> O backward pass √© um passo importante no treinamento de modelos de intelig√™ncia artificial. Nesse passo, **backward pass** √© o oposto do **forward pass**. Em vez de se mover para frente, o modelo se move para tr√°s, ajustando seus par√¢metros internos para reduzir a perda. Isso √© como o modelo dizendo: "Ah, eu errei! Vou tentar novamente e farei melhor!"

> [!NOTE]
> No Atualiza√ß√£o dos Par√¢metros durante o backward pass, o modelo atualiza seus par√¢metros internos, como pesos e bias de sua rede neural. Esses par√¢metros s√£o ajustados com base na diferen√ßa entre a sa√≠da prevista e a sa√≠da real. O objetivo √© encontrar o conjunto √≥timo de par√¢metros que minimize a perda.

Ap√≥s o terminus do treinamento ir√° aparecer na consola:
```
	TrainOutput(global_step=800, training_loss=0.2628884120285511, 
	metrics={'train_runtime': 692.0873, 'train_samples_per_second': 9.211, 
	'train_steps_per_second': 1.156, 'total_flos': 832883392512000.0, 
	'train_loss': 0.2628884120285511, 'epoch': 5.0})
```
Ele fornece informa√ß√µes sobre o estado do treinamento, incluindo:

* O passo global do treinamento (global_step): 800
* A perda de treinamento (training_loss): 0.2628884120285511
* M√©tricas de desempenho do treinamento, incluindo:
	+ Tempo de execu√ß√£o do treinamento (train_runtime): 692.0873 segundos
	+ N√∫mero de amostras por segundo (train_samples_per_second): 9.211
	+ N√∫mero de passos por segundo (train_steps_per_second): 1.156
	+ N√∫mero total de opera√ß√µes floating-point (total_flos): 832883392512000.0
	+ Perda de treinamento (train_loss): 0.2628884120285511
	+ √âpoca atual (epoch): 5.0

Essas informa√ß√µes podem ser √∫teis para monitorar o progresso do treinamento e ajustar os par√¢metros do modelo para melhorar o desempenho.

## üî® Salvar o modelo e tokenizador treinados

```python
	model.save_pretrained("./gpt2-chatbot")
	tokenizer.save_pretrained("./gpt2-chatbot")
```

Ap√≥s o treinamento do modelo, √© crucial salvar tanto o modelo quanto o tokenizador para reutiliza√ß√£o futura. Isso √© feito utilizando os m√©todos save_pretrained do GPT2LMHeadModel e GPT2Tokenizer, que armazenam os pesos treinados, configura√ß√µes, e vocabul√°rio em um diret√≥rio especificado, como ./gpt2-chatbot. Salvar esses componentes permite carreg√°-los posteriormente com from_pretrained, evitando a necessidade de retraining, facilitando o compartilhamento, backup e versionamento do modelo, garantindo efici√™ncia e consist√™ncia nas infer√™ncias futuras.

## üî® Carregar modelo e tokenizador treinados

```python
	model = GPT2LMHeadModel.from_pretrained("./gpt2-chatbot")
	tokenizer = GPT2Tokenizer.from_pretrained("./gpt2-chatbot")
```

Este passo envolve carregar o modelo e o tokenizador treinados usando a fun√ß√£o from_pretrained(), que permite reutilizar os pesos ajustados do modelo GPT-2 e as configura√ß√µes do tokenizador sem precisar retrainar. Isso √© essencial para aplica√ß√µes pr√°ticas, como responder perguntas dos usu√°rios em um aplicativo web. O c√≥digo carrega o modelo e o tokenizador do diret√≥rio ./gpt2-chatbot, e inclui uma fun√ß√£o gerar_resposta (passo 12) que tokeniza a entrada do usu√°rio, gera uma resposta com o modelo, e decodifica a sa√≠da para texto. Este processo garante infer√™ncias r√°pidas e consistentes com o treinamento.

## üî® Fun√ß√£o para gerar resposta

```python
	def gerar_resposta(model, tokenizer, input_text, max_length=50, num_return_sequences=1):
	    inputs = tokenizer.encode(input_text, return_tensors='pt')
	    attention_mask = [1] * len(inputs[0])
	    outputs = model.generate(inputs, attention_mask=torch.tensor([attention_mask]), max_length=max_length, num_return_sequences=num_return_sequences, pad_token_id=tokenizer.eos_token_id)
	    generated_text = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]
	    return generated_text
```

Esta √© uma fun√ß√£o Python que gera uma resposta baseada em um modelo de linguagem treinado. A fun√ß√£o √© chamada `gerar_resposta` e tem quatro par√¢metros:

* `model`: um modelo de linguagem treinado
* `tokenizer`: um objeto que √© usado para codificar e decodificar texto
* `input_text`: o texto de entrada que ser√° usado para gerar a resposta
* `max_length` (opcional): o tamanho m√°ximo da resposta gerada (padr√£o √© 50)
* `num_return_sequences` (opcional): o n√∫mero de respostas geradas (padr√£o √© 1)

Aqui est√° o que a fun√ß√£o faz:

1. Codifica o texto de entrada usando o objeto `tokenizer` e armazena o resultado em uma vari√°vel chamada `inputs`.
2. Cria uma m√°scara de aten√ß√£o* que √© usada para indicar quais tokens do texto de entrada devem ser considerados quando o modelo gera a resposta.
3. Chama o m√©todo `generate` do modelo para gerar a resposta. O m√©todo `generate` √© usado para gerar texto baseado em um texto de entrada e um modelo de linguagem.
4. O m√©todo `generate` retorna uma lista de sa√≠das, que s√£o as respostas geradas. A fun√ß√£o itera sobre essa lista e decodifica cada sa√≠da usando o objeto `tokenizer`.
5. A fun√ß√£o remove os tokens especiais (como tokens de in√≠cio e fim de texto) da resposta gerada usando o m√©todo `decode` do objeto `tokenizer`.
6. A fun√ß√£o retorna a lista de respostas geradas.

Em resumo, esta fun√ß√£o √© usada para gerar respostas baseadas em um modelo de linguagem treinado, com base em um texto de entrada.

> [!NOTE]
> M√°scara de aten√ß√£o* √© uma ferramenta usada em modelos de linguagem, especialmente em arquiteturas de transformadores, como GPT-2, para controlar quais tokens (palavras ou sub-palavras) em uma sequ√™ncia de entrada devem ser considerados (ou ‚Äúatendidos‚Äù) pelo modelo em diferentes etapas de processamento.


## üî® Exemplo de uso da fun√ß√£o de gera√ß√£o de resposta

```python
	input_text = "What is the term for a foul committed by a player that prevents an opponent from scoring a goal?"
	resposta = gerar_resposta(model, tokenizer, input_text)
	print(f"{resposta}")
```

Essa fun√ß√£o √© √∫til para criar chatbots e sistemas de resposta autom√°tica que necessitam de gera√ß√£o de texto baseado em modelos de linguagem.







